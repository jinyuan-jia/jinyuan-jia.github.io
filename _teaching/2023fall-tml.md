---
title: "Trustworthy Machine Learning"
collection: teaching
type: "Graduate course"
permalink: /teaching/2023fall-tml
venue: "Penn State, College of IST"
date: 2023-08-21
location: "State College, US"
---

## Overview
Machine learning techniques are widely used to solve real-world problems. However, a key challenge is that they are vulnerable to various security and privacy attacks, e.g., adversarial examples, data poisoning attacks, and membership inference attacks. In this course, we will discuss existing attacks and state-of-the-art defenses against those attacks.  

## Logistics

- Instructor: **Jinyuan Jia, jinyuan@psu.edu**
- Teaching Assistant: **Hangfan Zhang, hbz5148@psu.edu** 
- Time: **TuTh 3:05 PM - 4:20 PM**
- Location: **Leonhard Bldg 203**
- Office Hours: **Wednesday  1:00 pm - 2:00 pm, E325 Westgate**


## Tentative Schedule

| Week | Date | Topics | Papers | Notes |
| - | ---- | ------ | ------- | -------- |
| 1 | 08/22 | Course overview |  | |
| 1 | 08/24 |Adversarial examples in image domain (white-box)|1. Towards Evaluating the Robustness of Neural Networks <br/> ||
| 2 | 08/29 |Adversarial examples in image domain (black-box)|1. HopSkipJumpAttack: A Query-Efficient Decision-Based Attack <br/> 2. (Optional) Delving into Transferable Adversarial Examples and Black-box Attacks  ||
| 2 | 08/31 |Empirical defenses against adversarial examples| ||
| 3 | 09/05 |Certified defenses against adversarial examples| ||
| 3 | 09/07 |Adversarial examples in (large) language models | ||
| 4 | 09/12 | Certified defenses against adversarial examples in (large) language models | ||
| 4 | 09/14 | Data poisoning attacks to classifiers| ||
| 5 | 09/19 | Data poisoning attacks to foundation models| ||
| 5 | 09/21 | Model poisoning attacks to federated learning| ||
| 6 | 09/26 | Certified defenses against data poisoning attacks | ||
| 6 | 09/28 | Backdoor attacks in image domain | ||
| 7 | 10/03 | Defending against backdoor attacks in image domain | ||
| 7 | 10/05 | Backdoor attacks to (large) language models| ||
| 8 | 10/10 | Defending against backdoor attacks to (large) language models| ||
| 8 | 10/12 | Privacy attacks (membership inference attack) to image classifiers| ||
| 9 | 10/17 | Privacy attacks to federated learning| ||
| 9 | 10/19 | Privacy attacks (membership inference attack) to (large) language model| ||
| 10 | 10/24 | Defending against privacy attacks | ||
| 10 | 10/26 | Model stealing attacks| ||
| 11 | 10/31 | Defending against model stealing attacks| ||
| 11 | 11/02 | Intellectual property protection| ||
| 12 | 11/07 | Prompt injection attacks to large language models| ||
| 12 | 11/09 | Jailbreaking large language models | ||
| 13 | 11/14 | Deepfakes | ||
| 13 | 11/16 | Machine generated text detection| ||
| 14 | | Thanksgiving||
| 15 | 11/28 |Project Presentation| ||
| 15 | 11/30 |Project Presentation| ||
| 16 | 12/05 |Project Presentation| ||
| 16 | 12/07 |Project Presentation| ||

## Paper Review
- Deadline: Monday and Wednesday 11:59 pm (EST). Please send your review to this email address: XXX. Please send your review in a single thread (by replying).

## Group
- Students can form groups of at most 2 students for the lecture and class project.
- Lecture: Prepare the slides (or use others' with proper citations) and give a lecture. Choosing a topic for lecture. A group sends three preferred dates to XX by 11:59 pm (EST), 08/31.

- Class project: The project should be related to machine learning (published work cannot be used as the course project).

## Grading Policy
- 


